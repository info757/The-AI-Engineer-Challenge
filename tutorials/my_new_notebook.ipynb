{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099647b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import openai\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf8109a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beccd6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-C32ZlnSsP2MH0ANl3rAniV76U5rtu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"LangChain and LlamaIndex (formerly known as GPT Index) are both prominent frameworks designed to facilitate building applications that leverage large language models (LLMs), but they serve different purposes and have distinct functionalities. Here's a high-level comparison:\\n\\n### Purpose and Focus\\n\\n- **LangChain:**  \\n  Primarily a framework for developing complex, multi-step applications that involve chaining together LLM calls, integrating external data sources, APIs, and logic. It emphasizes building conversational agents, chatbots, and reasoning pipelines with modular components.\\n\\n- **LlamaIndex (GPT Index):**  \\n  Focuses on integrating external data sources (like documents, databases, knowledge bases) with LLMs to enable effective retrieval-augmented generation (RAG). It helps in indexing and querying large unstructured data, making it easier to build applications such as document Q&A systems.\\n\\n### Core Functionality\\n\\n- **LangChain:**  \\n  - Chains: Compose multiple LLM calls in sequence or in complex workflows.  \\n  - Memory: Maintain context across interactions.  \\n  - Tools/Agents: Enable LLMs to invoke external tools or APIs dynamically.  \\n  - Data Connectors: Connect to external sources to fetch data during the chain.  \\n  - Supports multiple models and integrations with various APIs.\\n\\n- **LlamaIndex:**  \\n  - Indexing: Create structured indices over unstructured data (e.g., documents, PDFs).  \\n  - Querying: Perform efficient retrievals to answer questions based on indexed data.  \\n  - Data ingestion pipelines: Simplify loading and preprocessing of large datasets.  \\n  - Designed to enhance performance and accuracy of retrieval-augmented generation.\\n\\n### Use Cases\\n\\n- **LangChain:**  \\n  - Building chatbots, virtual assistants, reasoning pipelines, and agents that perform complex tasks.  \\n  - Orchestrating multiple LLM calls with control logic.  \\n  - Integrating external APIs and data sources during interactions.\\n\\n- **LlamaIndex:**  \\n  - Creating knowledge bases from large collections of documents.  \\n  - Building question-answering systems over large datasets.  \\n  - Implementing retrieval-augmented generation to supplement LLM outputs with external knowledge.\\n\\n### Integration and Ecosystem\\n\\n- **LangChain:**  \\n  - Supports multiple LLM providers (OpenAI, Hugging Face, Azure).  \\n  - Offers a rich set of tools and integrations for building applications.  \\n  - Python-centric, with an active community and extensibility.\\n\\n- **LlamaIndex:**  \\n  - Focused on data ingestion and retrieval over external datasets.  \\n  - Compatible with various data sources and retrieval mechanisms.  \\n  - Also Python-based, often used in conjunction with LLMs like GPT.\\n\\n---\\n\\n### Summary\\n\\n| Aspect                       | **LangChain**                                            | **LlamaIndex**                                              |\\n|------------------------------|----------------------------------------------------------|--------------------------------------------------------------|\\n| Main purpose                 | Building complex LLM applications, pipelines, and agents | Indexing and querying large external datasets for augmentation |\\n| Focus                        | Workflow orchestration, chaining, tool integration       | Data ingestion, indexing, retrieval for knowledge bases   |\\n| Typical use cases            | Chatbots, reasoning systems, multi-step workflows       | Document QA, knowledge retrieval, retrieval-augmented generation |\\n| Extensibility                | Highly modular, supports complex workflows              | Specialized in data indexing and retrieval                |\\n\\n---\\n\\n### In Summary\\n\\n- **Choose LangChain** if you're interested in developing multi-step applications, agents, or chatbots that orchestrate various LLMs and tools.\\n- **Choose LlamaIndex** if you need to build systems that incorporate large document stores or external data sources, enabling LLMs to retrieve and leverage external knowledge effectively.\\n\\nThey are often complementary; you might use LlamaIndex for data retrieval within a broader LangChain-powered application.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754840921, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_f12167b370', usage=CompletionUsage(completion_tokens=773, prompt_tokens=19, total_tokens=792, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
    "\n",
    "client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea93e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"developer\", \"content\": message}\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"assistant\", \"content\": message}\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"user\", \"content\": message}\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f905bff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Great question! LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks in the AI and natural language processing ecosystem, but they serve different primary purposes and have distinct features. Here's a comparison to clarify their differences:\n",
       "\n",
       "**1. Purpose and Use Cases**\n",
       "\n",
       "- **LangChain:**\n",
       "  - Focuses on building complex, multi-step, conversational AI applications.\n",
       "  - Provides tools for chaining together language model prompts, managing memory, and integrating with various data sources and APIs.\n",
       "  - Suitable for developing chatbots, virtual assistants, or applications requiring dynamic interactions and reasoning.\n",
       "\n",
       "- **LlamaIndex (GPT Index):**\n",
       "  - Designed primarily for indexing and querying large amounts of external data (documents, PDFs, notes, etc.) using language models.\n",
       "  - Facilitates building retrieval-augmented generation (RAG) pipelines, where LLMs answer questions based on indexed data.\n",
       "  - Ideal for creating knowledge bases, document retrieval systems, and knowledge management tools.\n",
       "\n",
       "**2. Core Functionality**\n",
       "\n",
       "- **LangChain:**\n",
       "  - Provides abstractions for prompt management, memory, and chains of operations involving language models.\n",
       "  - Supports integration with a variety of data sources, APIs, and tools to enable complex workflows.\n",
       "  - Emphasizes the orchestration of language model calls and managing conversational context.\n",
       "\n",
       "- **LlamaIndex:**\n",
       "  - Offers data ingestion and indexing mechanisms to organize large datasets for efficient retrieval.\n",
       "  - Implements various indices (e.g., list, tree, vector) to optimize search and query performance.\n",
       "  - Incorporates retrieval techniques to enhance the accuracy and relevance of language model responses.\n",
       "\n",
       "**3. Design and Architecture**\n",
       "\n",
       "- **LangChain:**\n",
       "  - Modular and extensible framework centered around chains, prompts, memory, and tools.\n",
       "  - Focuses on composing prompt workflows and managing complex interactions.\n",
       "\n",
       "- **LlamaIndex:**\n",
       "  - Data-centric, emphasizing the preprocessing, indexing, and querying of external data.\n",
       "  - Designed to work seamlessly with language models to produce context-aware answers.\n",
       "\n",
       "**4. Ecosystem and Integration**\n",
       "\n",
       "- **LangChain:**\n",
       "  - Supports multiple LLM providers (OpenAI, Hugging Face, Azure, etc.).\n",
       "  - Offers integrations with databases, APIs, and toolkits for building end-to-end AI applications.\n",
       "\n",
       "- **LlamaIndex:**\n",
       "  - Focuses on integration with document stores and vector databases.\n",
       "  - Connects with various LLM providers to perform retrieval-augmented tasks.\n",
       "\n",
       "---\n",
       "\n",
       "### Summary Table:\n",
       "\n",
       "| Aspect                     | LangChain                                           | LlamaIndex (GPT Index)                        |\n",
       "|----------------------------|-----------------------------------------------------|----------------------------------------------|\n",
       "| Main Purpose               | Building conversational AI workflows and applications | Indexing and querying large datasets for knowledge retrieval |\n",
       "| Core Functionality         | Chains, prompts, memory, tool integrations         | Data ingestion, indexing, retrieval        |\n",
       "| Typical Use Cases          | Chatbots, virtual assistants, multi-step workflows | Document QA systems, knowledge bases       |\n",
       "| Data Handling              | Manages interactions, context, and tool invocation | Organizes external data for efficient querying |\n",
       "| Integration Focus          | APIs, tools, multiple LLM providers                  | Data sources, document stores, vector DBs |\n",
       "\n",
       "---\n",
       "\n",
       "**In essence:**  \n",
       "- Use **LangChain** if you're building complex AI applications with dynamic interactions, workflows, and integrations.  \n",
       "- Use **LlamaIndex** if you're organizing large collections of documents or data sources to enable efficient question-answering or retrieval-based applications.\n",
       "\n",
       "Let me know if you'd like more detailed examples or guidance on choosing between them!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [user_prompt(YOUR_PROMPT)]\n",
    "\n",
    "chatgpt_response = get_response(client, messages)\n",
    "\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "909b5a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Are you kidding me? Neither! I don't have time to deal with this ice nonsense when I'm starving. Just give me my food already!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are irate and extremely hungry.\"),\n",
    "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
    "]\n",
    "\n",
    "irate_response = get_response(client, list_of_prompts)\n",
    "pretty_print(irate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55853c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I think both types of ice have their charm! Crushed ice is great for making drinks extra refreshing and chilled quickly, like in a mint julep or a snow cone. Cubed ice, on the other hand, keeps your beverage cold longer without diluting it too fast—perfect for whiskey or soda. Which do you prefer?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
    "\n",
    "joyful_response = get_response(client, list_of_prompts)\n",
    "pretty_print(joyful_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "035487ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-C33Ak4OCNskLTjlDkAR7evvoNSa71', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I think both types of ice have their charm! Crushed ice is great for making drinks extra refreshing and chilled quickly, like in a mint julep or a snow cone. Cubed ice, on the other hand, keeps your beverage cold longer without diluting it too fast—perfect for whiskey or soda. Which do you prefer?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754843214, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=67, prompt_tokens=30, total_tokens=97, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(joyful_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dec1d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Here's a sentence using the words 'stimple' and 'falbean': \n",
       "\n",
       "\"Despite the stimple challenges, she remained optimistic, especially after discovering a falbean solution that simplified the process.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd650efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The stimple wrench easily tightened the bolt around the falbean to ensure a secure fit."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
    "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
    "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ab00c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's analyze the options:\n",
       "\n",
       "**Option 1: Fly + Bus**  \n",
       "- Flying time: 3 hours  \n",
       "- Bus time: 2 hours  \n",
       "- Total travel time: 3 + 2 = 5 hours\n",
       "\n",
       "**Option 2: Teleporter + Bus**  \n",
       "- Teleporter time: 0 hours  \n",
       "- Bus time: 1 hour  \n",
       "- Total travel time: 0 + 1 = 1 hour\n",
       "\n",
       "---\n",
       "\n",
       "**Current local time:** 1 PM  \n",
       "**Target arrival time:** Before 7 PM EDT\n",
       "\n",
       "Assuming the local time is in the same timezone as Billy's current position (i.e., the starting point), and considering that he needs to arrive before 7 PM EDT:\n",
       "\n",
       "- Since it is currently 1 PM local time, and he needs to arrive before 7 PM EDT, we must consider if any time zone differences matter.  \n",
       "- If the current local time and the target time are in different zones, we need to account for that.  \n",
       "- Typically, San Francisco is in Pacific Time (PT), which is UTC-8 or UTC-7 during daylight saving (PDT), and EDT is UTC-4.\n",
       "\n",
       "**Assuming daylight saving (which is typical in October):**  \n",
       "- San Francisco: PDT (UTC-7)  \n",
       "- Eastern Time: EDT (UTC-4)\n",
       "\n",
       "**Time difference:**  \n",
       "- PDT is 3 hours behind EDT.\n",
       "\n",
       "So, 1 PM PDT corresponds to 4 PM EDT.\n",
       "\n",
       "**Calculating the arrival deadline in local time:**  \n",
       "- Billy needs to arrive before 7 PM EDT, which is 7 PM EDT.  \n",
       "- Since 1 PM PDT corresponds to 4 PM EDT, he needs to arrive by 7 PM EDT.  \n",
       "- From 4 PM EDT to 7 PM EDT is 3 hours.\n",
       "\n",
       "**Now, check if the travel times fit within this window:**\n",
       "\n",
       "- **Option 1:** 5 hours total  \n",
       "  - Starting at 1 PM PDT (4 PM EDT): arrive at 4 PM + 5 hours = 9 PM EDT, **which is too late**.\n",
       "  - He would arrive after 7 PM EDT.\n",
       "\n",
       "- **Option 2:** 1 hour total  \n",
       "  - Starting at 1 PM PDT (4 PM EDT): arrive at 4 PM + 1 hour = 5 PM EDT, which is **before** 7 PM.\n",
       "\n",
       "---\n",
       "\n",
       "**Conclusion:**  \n",
       "- If Billy wants to arrive **before 7 PM EDT**, he should **choose the teleporter + bus option**.  \n",
       "- The fly + bus option would make him arrive too late (around 9 PM EDT).\n",
       "\n",
       "**Does it matter which option he selects?**  \n",
       "**Yes,** because only the teleporter + bus allows arrival before the 7 PM EDT deadline.\n",
       "\n",
       "---\n",
       "\n",
       "**Final answer:**  \n",
       "Yes, it matters. To get home before 7 PM EDT, Billy should choose the teleporter + bus option."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoning_problem = \"\"\"\n",
    "Billy wants to get home from San Fran. before 7PM EDT.\n",
    "\n",
    "It's currently 1PM local time.\n",
    "\n",
    "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
    "\n",
    "Does it matter which travel option Billy selects?\n",
    "\"\"\"\n",
    "\n",
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem)\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
